{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since some the NLP models are quite huge they greatly benefit from using a GPU during the training phase, we're going to use Google Colab for this exercise. You can read this blog [post](https://www.dataquest.io/blog/getting-started-with-google-colab-for-deep-learning?_gl=1*1svxc04*_gcl_au*MTExNjMxNDExNS4xNzA0MjE3NTA5) to learn more about it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxz83pZ3ky0S"
      },
      "source": [
        "# 2. TensorFlow Datasets (TFDS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TensorFlow Datasets (TFDS) is a library of utilities that simplify preprocessing, loading, and analyzing data in TensorFlow. It offers high-level APIs to create reproducible input pipelines for TensorFlow models. Additionally, TFDS supports many popular public datasets, making it easy for developers to quickly test their models on real-world data. Moreover, TFDS supports cloud-based data storage systems, such as Google Cloud Storage (GCS), and Amazon S3, allowing developers to securely access large datasets with ease. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This will load the \"train\" set of the `imbd_reviews` dataset as opposed to the \"test\" set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q70xqTxNlXB2",
        "outputId": "0bbeb4ca-5295-4ab8-d528-9921eada90a3"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "imdb_data = tfds.load(name=\"imdb_reviews\", split=\"train\")\n",
        "imdb_df = tfds.as_dataframe(imdb_data)\n",
        "print(f\"Shape of data: {imdb_df.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can generate the list of available datasets with the following command"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tfds.list_builders()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlTNhFh9k6IR"
      },
      "source": [
        "# 3. Exploring the IMDB Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The IMDB dataset contains 50,000 movie review texts categorized as either positive or negative. You can learn more about the dataset [here](http://ai.stanford.edu/~amaas/data/sentiment/)\n",
        "- The IMDB dataset is evenly split between the \"train\" and \"test\" sets â€“ each containing 25,000 observations. So if we call the `tfds.load()` function and pass it the `split=\"train\"` or `split=\"test\"` argument, the results will be different datasets of 25,000 records each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrtCuqbWlXvU"
      },
      "outputs": [],
      "source": [
        "imdb_df['text'] = imdb_df['text'].str.decode('utf-8')\n",
        "imdb_sample = imdb_df.sample(frac=0.2, random_state=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjr8uvTetn_b",
        "outputId": "1bfcafae-96ae-4ca0-8a2a-6f24cf28255c"
      },
      "outputs": [],
      "source": [
        "print(f'Shape of sample: {imdb_sample.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "qD9VxgW2s52b",
        "outputId": "52bab2a8-0a38-43db-e166-6a239e952314"
      },
      "outputs": [],
      "source": [
        "imdb_sample.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "t4_Ot34ns8PB",
        "outputId": "970051c3-f225-4bf8-d06a-9cbd1a9b5b0c"
      },
      "outputs": [],
      "source": [
        "imdb_sample.tail(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The distribution of the target variable: `label`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dC6RMNus-aZ",
        "outputId": "8b2da8dc-7cb5-4d61-f9e3-1bf572a9c1ae"
      },
      "outputs": [],
      "source": [
        "imdb_sample['label'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Positive and negative movie reviews are indicated by the `label` values 1 and 0, respectively. As we can see in the above output, this is a balanced dataset with an equal number of positive and negative reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxqNEHUvtAMz",
        "outputId": "4f716fcc-0c62-4671-ea07-870a4923c898"
      },
      "outputs": [],
      "source": [
        "imdb_sample.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igp52tufk6AM"
      },
      "source": [
        "# 4. Word Distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Understanding the distribution of text lengths is important:\n",
        "- It can provide insight into a reviewer's messaging strategy. For instance, if most of their reviews are short, it could mean they are aiming for brevity and efficiency. Conversely, if they write longer reviews, it could indicate they are looking to provide detailed information or engage in meaningful dialogue.\n",
        "- There may be a case where the length of the review is indicative of a positive or negative review. Thus, we can test this hypothesis by comparing statistical values related to each group to see if there is a correlation between the length of a review and its label. We can create a new variable, `text_length`, in the dataframe `df` with the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "T3RJp7eblYU0",
        "outputId": "907023f6-281b-4c4c-b9dc-9ba4f4e6ca2d"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "imdb_sample['text_length'] = [len(review.split(' ')) for review in imdb_sample['text']]\n",
        "sns.histplot(data=imdb_sample, x='text_length', bins=30)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To understand the distribution of `text_length` for both positive and negative reviews, we can calculate the mean of `text_length` grouped by the target variable, `label`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "ESDYL1i9z3y1",
        "outputId": "0f00319d-261b-460e-828f-1b2e9612d969"
      },
      "outputs": [],
      "source": [
        "imdb_sample.groupby(by=\"label\")[[\"text_length\"]].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "9KUc0-E2xVPs",
        "outputId": "400ea25e-3bae-4850-879e-322c97e8fc8e"
      },
      "outputs": [],
      "source": [
        "imdb_sample.groupby(by=\"label\")[[\"text_length\"]].median()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "HmUNA87TxbB6",
        "outputId": "470c36f5-75e6-4c4a-f383-72b62fc9b8bd"
      },
      "outputs": [],
      "source": [
        "imdb_sample.groupby(by=\"label\")[[\"text_length\"]].std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "9T25cxBF0nne",
        "outputId": "7606efe6-2f98-4b90-c58a-4646896c3480"
      },
      "outputs": [],
      "source": [
        "# Optional bar plot code\n",
        "mean_length = imdb_sample.groupby(by=\"label\")[[\"text_length\"]].mean().reset_index()\n",
        "median_length = imdb_sample.groupby(by=\"label\")[[\"text_length\"]].median().reset_index()\n",
        "std_length = imdb_sample.groupby(by=\"label\")[[\"text_length\"]].std().reset_index()\n",
        "\n",
        "combined_df = pd.concat([mean_length, median_length[\"text_length\"], std_length[\"text_length\"]], axis=1)\n",
        "combined_df.columns = [\"label\", \"mean_length\", \"median_length\", \"std_length\"]\n",
        "melted_df = pd.melt(combined_df, id_vars=\"label\", var_name=\"Measure\", value_name=\"Value\")\n",
        "\n",
        "sns.barplot(data=melted_df, x=\"label\", y=\"Value\", hue=\"Measure\")\n",
        "plt.xlabel(\"Label\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.title(\"Comparison of Measures\")\n",
        "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EuaBg0PxndN"
      },
      "source": [
        "Based on the results above, there doesn't appear to be a correlation between `text_length` and `label`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyL6RN0pk51E"
      },
      "source": [
        "# 5. Visualizing the Most Frequent Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualizing the most frequently used words is critical:\n",
        "- It gives us an idea of the general theme or tone of the dataset. This can help us understand its purpose and potential applications\n",
        "- It help us spot errors in the data. For example, if we see that stopwords are being used a lot in the dataset, it could be a sign that we should eliminate them during text preprocessing. Stopwords are words that are commonly used in a language, but have little semantic meaning (\"a\", \"an\", \"the\", \"of\", \"to\", and \"in\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HI9p22DblZAa",
        "outputId": "9b11ebb3-8407-4dd1-dfb1-3c74ca45e37d"
      },
      "outputs": [],
      "source": [
        "freq_words = imdb_sample['text'].str.split(expand=True).stack().value_counts()\n",
        "freq_words_top100 = freq_words[:100]\n",
        "freq_words_top100.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A treemap is a great way to visualize frequently used words by arranging them in rectangles of varying size. The size of each rectangle is directly proportional to the frequency of that word in a given text or dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "Zr_0dtb2F_Xd",
        "outputId": "db04a0c7-b515-4029-a226-c871f9ed63ac"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "fig = px.treemap(freq_words_top100, path=[freq_words_top100.index], values=0)\n",
        "fig.update_layout(title_text='Most Frequent 100 Words in the Dataset', title_font=dict(size=20))\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXnAUOpdk5mR"
      },
      "source": [
        "# 6. Text Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preprocessing text data is crucial for several reasons. It is a necessary step in any natural language processing (NLP) task, as it helps to clean and standardize the text, making it easier for the NLP algorithm to process. Furthermore, preprocessing can improve the accuracy of our results.\n",
        "\n",
        "Preprocessing involves several steps that can help improve the overall quality of our text data. These steps include:\n",
        "\n",
        "1. Coverting the text to lowercase\n",
        "2. Removing punctuation from the text\n",
        "3. Tokenizing the text\n",
        "4. Removing stopwords from the text\n",
        "5. Lemmatization (Stemming) of the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "M8UpZ_CUlZvS",
        "outputId": "4fbf16a7-6950-49ee-cbea-04077481079e"
      },
      "outputs": [],
      "source": [
        "imdb_sample['text'] = imdb_sample['text'].str.lower()\n",
        "\n",
        "import re\n",
        "\n",
        "def punctuation(inputs):\n",
        "    return re.sub(r'[^\\w\\s]', ' ', inputs)\n",
        "\n",
        "imdb_sample['text'] = imdb_sample['text'].apply(punctuation)\n",
        "\n",
        "imdb_sample.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuwAqTswlaTs",
        "outputId": "5f0eff71-ea52-4dbf-a3cd-245157ed9a39"
      },
      "outputs": [],
      "source": [
        "# Tokenization\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def tokenization(inputs):\n",
        "    return word_tokenize(inputs)\n",
        "\n",
        "imdb_sample['text_tokenized'] = imdb_sample['text'].apply(tokenization)\n",
        "imdb_sample['text_tokenized'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVqpQ38vm9R2",
        "outputId": "a87b3f15-d3af-409c-d971-dbd6b02bd67b"
      },
      "outputs": [],
      "source": [
        "# Stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.remove('not')\n",
        "stop_words.add('br')\n",
        "\n",
        "def stopwords_remove(inputs):\n",
        "    return [word for word in inputs if word not in stop_words]\n",
        "\n",
        "imdb_sample['text_stop'] = imdb_sample['text_tokenized'].apply(stopwords_remove)\n",
        "imdb_sample['text_stop'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFIReyqQnCVO",
        "outputId": "facecce6-269f-4280-ba0f-3a3aa2f5897b"
      },
      "outputs": [],
      "source": [
        "# Lemmatization\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatization(inputs):\n",
        "    return [lemmatizer.lemmatize(word=word, pos='v') for word in inputs]\n",
        "\n",
        "imdb_sample['text_lemmatized'] = imdb_sample['text_stop'].apply(lemmatization)\n",
        "imdb_sample['text_lemmatized'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INMpJqnPk5Lu"
      },
      "source": [
        "# 8. Visualization of Reviews after Text Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LetÂ´s go with mmost frequently used 100 words after text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "KtL_YMuErwmR",
        "outputId": "7ef7b15e-1b90-4a77-9a45-9893bca3e86c"
      },
      "outputs": [],
      "source": [
        "imdb_sample['final'] = imdb_sample['text_lemmatized'].str.join(' ')\n",
        "\n",
        "freq_words = imdb_sample['final'].str.split(expand=True).stack().value_counts()\n",
        "freq_words_top100 = freq_words[:100]\n",
        "\n",
        "fig = px.treemap(freq_words_top100, path=[freq_words_top100.index], values=0)\n",
        "fig.update_layout(title_text='Most Frequently Used 100 Words after Text Preprocessing', title_font=dict(size=20))\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A WordCloud is a graphical representation of the most common words used in a piece of text. The more often a word is used, the larger it appears in the word cloud visualization. A WordCloud can be used to quickly and easily identify the most important themes in a text. Let's visualize just the positive reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "kh054G1Vla3z",
        "outputId": "4f89528b-1646-4b40-9ae5-87f91832a79b"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "# Negative movie reviews\n",
        "imdb_sample_0 = imdb_sample[imdb_sample['label'] == 0]\n",
        "word_cloud_0 = WordCloud(max_words=100, stopwords=stop_words, random_state=100).generate(' '.join(imdb_sample_0['final'].tolist()))\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.imshow(word_cloud_0, interpolation='bilinear')\n",
        "plt.title('WordCloud of Frequently Used Words in Negative Reviews', fontsize=20)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "P_0wAAszr1tV",
        "outputId": "b970ebb0-5f61-4a04-a157-68687ef285e7"
      },
      "outputs": [],
      "source": [
        "# Positive movie reviews\n",
        "imdb_sample_1 = imdb_sample[imdb_sample['label'] == 1]\n",
        "word_cloud_1 = WordCloud(max_words=100, stopwords=stop_words, random_state=100).generate(' '.join(imdb_sample_1['final'].tolist()))\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.imshow(word_cloud_1, interpolation='bilinear')\n",
        "plt.title('WordCloud of Frequently Used Words in Positive Reviews', fontsize=20)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Surprisingly, it seems that positive and negative movie reviews tend to use similar words. For example, both word clouds are dominated by words like: ['film', 'movie', 'one', 'not', 'like', 'make', 'see', 'get', 'character'] which makes sense when we look at our treemap from the previous screen that shows the most frequently used words, regardless of whether they are positive or negative reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For your placeÂ´s warm: If we wanted to create word clouds that are more distinct between positive and negative reviews, we could use the `stopwords` argument of the `WordCloud()` constructor to specifically remove some of these shared top-words and generate the word clouds again. Feel free to experiment!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
